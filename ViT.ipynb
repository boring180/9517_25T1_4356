{"cells":[{"cell_type":"markdown","metadata":{"id":"Yo_DNvNUDsiV"},"source":["# Import necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1lQ-wNFDsiZ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import os\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","\n","from torchvision.transforms import ToTensor\n","from torch.utils.tensorboard import SummaryWriter\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device: \", device)"]},{"cell_type":"markdown","metadata":{"id":"dZ741-9pDsia"},"source":["# Load the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0u46qOV2Dsia"},"outputs":[],"source":["folder_path = 'normal_train/'\n","\n","data = []\n","\n","# for f in os.listdir(folder_path):\n","#     if f.endswith('.png'):\n","#         img = plt.imread(os.path.join(folder_path, f))\n","#         img_class = f.split('.')[0].split('_')[0]\n","#         data.append({'image':img, 'class':img_class})\n","#         print(folder_path + f)\n","\n","# df = pd.DataFrame(data)\n","\n","# with open('data.pkl', 'wb') as f:\n","#     pickle.dump(df, f)\n","\n","with open('data.pkl', 'rb') as f:\n","    df = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4v-hxcdDsia"},"outputs":[],"source":["folder_path = 'normal_test/'\n","\n","test_data = []\n","\n","# for f in os.listdir(folder_path):\n","#     if f.endswith('.png'):\n","#         img = plt.imread(os.path.join(folder_path, f))\n","#         img_class = f.split('.')[0].split('_')[0]\n","#         test_data.append({'image':img, 'class':img_class})\n","#         print(folder_path + f)\n","\n","# test_df = pd.DataFrame(test_data)\n","\n","# with open('test_data.pkl', 'wb') as f:\n","#     pickle.dump(test_df, f)\n","\n","with open('test_data.pkl', 'rb') as f:\n","    test_df = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"nntONQyEDsib"},"source":["# Display the first image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sXfXYxwDsib"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","first_image = df['image'].iloc[0]\n","first_class = df['class'].iloc[0]\n","\n","plt.figure(figsize=(5, 5))\n","plt.imshow(first_image)\n","plt.title(f'Class: {first_class}')\n","plt.axis('off')\n","plt.show()\n","\n","print(first_image.shape)"]},{"cell_type":"markdown","metadata":{"id":"Adz34eF_Dsic"},"source":["# CustomDataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ym15awEvDsid"},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        image = self.df['image'].iloc[idx]\n","        label = self.df['class'].iloc[idx]\n","\n","        image = torch.from_numpy(image).float()\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"nBw8kHX3Dsid"},"source":["# Constants & Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2E684_FJDsid"},"outputs":[],"source":["n_image = 9600\n","chw = (4, 256, 256)\n","\n","n_patches=16 # Number of patches in each row and column\n","n_blocks=4\n","hidden_d=64 # Hidden dimension in patch embedding(token dimension)\n","n_heads=8 # Number of heads in multi-head self-attention, must be divisible by hidden_d\n","out_d=15 # Number of classes\n","mlp_ratio = 16\n","N_EPOCHS = 200\n","LR = 1e-3\n","BATCH_SIZE = 512"]},{"cell_type":"markdown","metadata":{"id":"sf0V-1rDDsie"},"source":["# Train-Validation Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SC-Vcf_UDsie"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","le.fit(df['class'])\n","with open('le.pkl', 'wb') as f:\n","    pickle.dump(le, f)\n","df['class'] = le.transform(df['class'])\n","test_df['class'] = le.transform(test_df['class'])\n","\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","transform = ToTensor()\n","train_set = CustomDataset(train_df)\n","val_set = CustomDataset(val_df)\n","test_set = CustomDataset(test_df)\n","\n","train_loader = DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n","val_loader = DataLoader(val_set, shuffle=True, batch_size=BATCH_SIZE)\n","test_loader = DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n"]},{"cell_type":"markdown","metadata":{"id":"z7Tf2tv1Dsie"},"source":["# Patchify"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdDiVPyjDsie"},"outputs":[],"source":["def patchify(images):\n","    n, h, w, c = images.shape\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2, device=images.device) # Nn p^2, HWC / p^2\n","    patch_size = h // n_patches\n","\n","    for idx in range(n):\n","        for i in range(n_patches):\n","            for j in range(n_patches):\n","                patch = images[idx, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size, :]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","\n","    return patches"]},{"cell_type":"markdown","metadata":{"id":"d5nZHL9PDsie"},"source":["# Positional Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHH3WY1KDsie"},"outputs":[],"source":["def get_positional_embeddings(sequence_length, d):\n","    result = torch.ones(sequence_length, d)\n","    for i in range(sequence_length):\n","        for j in range(d):\n","            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"TCH-jfV4Dsie"},"source":["# Multi-Head Self Attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"997jSUwIDsie"},"outputs":[],"source":["class MyMSA(nn.Module):\n","    def __init__(self):\n","        super(MyMSA, self).__init__()\n","\n","        self.n_heads = n_heads\n","        d_head = int(hidden_d / n_heads)\n","        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.d_head = d_head\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, sequences):\n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","\n","                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n","                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n","\n","                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n","                seq_result.append(attention @ v)\n","            result.append(torch.hstack(seq_result))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"markdown","metadata":{"id":"goxPi3hUDsif"},"source":["# ViT Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JotgnrzcDsif"},"outputs":[],"source":["class MyViTBlock(nn.Module):\n","    def __init__(self):\n","        super(MyViTBlock, self).__init__()\n","\n","        self.norm1 = nn.LayerNorm(hidden_d)\n","        self.mhsa = MyMSA()\n","        self.norm2 = nn.LayerNorm(hidden_d)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n","        )\n","\n","    def forward(self, x):\n","        out = x + self.mhsa(self.norm1(x))\n","        out = out + self.mlp(self.norm2(out))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"X9EawmhuDsif"},"source":["# Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pqcf-iNLDsif"},"outputs":[],"source":["class MyViT(nn.Module):\n","  def __init__(self):\n","    # Super constructor\n","    super(MyViT, self).__init__()\n","    self.batch_size = BATCH_SIZE\n","\n","    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n","\n","    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n","    self.linear_mapper = nn.Linear(self.input_d, hidden_d)\n","\n","    self.class_token = nn.Parameter(torch.rand(1, hidden_d))\n","\n","    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(n_patches ** 2 + 1, hidden_d)))\n","    self.pos_embed.requires_grad = False\n","\n","    self.blocks = nn.ModuleList([MyViTBlock() for _ in range(n_blocks)])\n","\n","    self.mlp = nn.Sequential(\n","            nn.Linear(hidden_d, out_d),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","\n","  def forward(self, images):\n","    self.batch_size = images.shape[0]\n","    patches = patchify(images)\n","    tokens = self.linear_mapper(patches)\n","    tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n","    pos_embed = self.pos_embed.repeat(self.batch_size, 1, 1)\n","    out = tokens + pos_embed\n","    for block in self.blocks:\n","        out = block(out)\n","\n","    out = out[:, 0]\n","\n","    return self.mlp(out)"]},{"cell_type":"markdown","metadata":{"id":"7gf97OpnDsif"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_WISzkEDsif"},"outputs":[],"source":["model = MyViT().to(device)\n","optimizer = Adam(model.parameters(), lr=LR)\n","criterion = CrossEntropyLoss()\n","max_acc = 0.0\n","\n","writer = SummaryWriter()\n","\n","for epoch in range(N_EPOCHS):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for batch in train_loader:\n","        x, y = batch\n","        x, y = x.to(device), y.to(device)\n","        y_hat = model(x)\n","\n","        loss = criterion(y_hat, y)\n","        train_loss += loss.detach().cpu().item() / len(train_loader)\n","\n","        _, predicted = torch.max(y_hat.data, 1)\n","        total += y.size(0)\n","        correct += (predicted == y).sum().item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    train_acc = 100 * correct / total\n","\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    val_loss = 0.0\n","    for batch in val_loader:\n","        x, y = batch\n","        x, y = x.to(device), y.to(device)\n","        y_hat = model(x)\n","\n","        loss = criterion(y_hat, y)\n","        val_loss += loss.detach().cpu().item() / len(val_loader)\n","\n","        _, predicted = torch.max(y_hat.data, 1)\n","        total += y.size(0)\n","        correct += (predicted == y).sum().item()\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Epoch {epoch + 1}/{N_EPOCHS} val_accuracy: {val_acc:.2f}%\")\n","    writer.add_scalar('Loss/train', train_loss, epoch)\n","    writer.add_scalar('Loss/validation', val_loss, epoch)\n","    writer.add_scalar('Accuracy/train', train_acc, epoch)\n","    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n","\n","    if max_acc < val_acc:\n","        max_acc = val_acc\n","        torch.save(model, 'ViT.pth')\n"]},{"cell_type":"markdown","metadata":{"id":"Zg2yRB4XDsif"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3rQhuN-Dsif"},"outputs":[],"source":["from PIL import Image\n","from torchvision import transforms\n","transform = transforms.ToPILImage()\n","\n","\n","model = torch.load('ViT.pth').to(device)\n","model.eval()\n","correct = 0\n","total = 0\n","data = []\n","\n","os.makedirs('false', exist_ok=True)\n","count = 0;\n","\n","\n","for batch in test_loader:\n","    x, y = batch\n","    x, y = x.to(device), y.to(device)\n","    y_hat = model(x)\n","\n","    _, predicted = torch.max(y_hat.data, 1)\n","    total += y.size(0)\n","    correct += (predicted == y).sum().item()\n","\n","    data.extend([{'y': y_item.item(), 'y_hat': pred_item.item()} for y_item, pred_item in zip(y, predicted)])\n","\n","    for i in range(y.size(0)):\n","        if y[i]!=predicted[i]:\n","            image = x[i].permute(2, 1, 0)\n","            image = transform(image)\n","            image_class = le.inverse_transform(np.array([predicted[i].cpu().item()]))\n","            image_path = os.path.join('false', f'false_{image_class}_{count}.png')\n","            image.save(image_path)\n","            count += 1\n","\n","test_acc = 100 * correct / total\n","df = pd.DataFrame(data)\n","print(df)\n","df.to_csv('predictions.csv', index=False)\n","print(test_acc)\n"]},{"cell_type":"markdown","metadata":{"id":"iTpx2LteDsif"},"source":["# Reference:\n","\n","https://keras.io/examples/vision/image_classification_with_vision_transformer/  \n","https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}